#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è§†é¢‘å…ƒæ•°æ®æå–è„šæœ¬

ä»æœ¬åœ°è§†é¢‘æ–‡ä»¶å’Œ F2 æ•°æ®åº“æå–è§†é¢‘å…ƒæ•°æ®å¹¶ä¿å­˜åˆ° SQLite æ•°æ®åº“ã€‚

ç”¨æ³•ï¼š
    python scripts/extract-metadata.py              # æ‰«ææœ¬åœ°è§†é¢‘å¹¶æå–å…ƒæ•°æ®
    python scripts/extract-metadata.py --stats      # æ˜¾ç¤ºç»Ÿè®¡æ‘˜è¦

âš ï¸ DEPRECATED: --fetch é€‰é¡¹å·²åºŸå¼ƒ
æ¨èä½¿ç”¨ download-v2.py é‡æ–°ä¸‹è½½è§†é¢‘ï¼Œä¼šè‡ªåŠ¨ä¿å­˜ç»Ÿè®¡æ•°æ®ï¼š
    python scripts/download-v2.py <ä¸»é¡µURL>

åŸå› ï¼š
- --fetch ä½¿ç”¨æµè§ˆå™¨æ–¹å¼è·å–æ•°æ®ï¼Œå¯èƒ½è§¦å‘åçˆ¬
- download-v2.py åœ¨ä¸‹è½½æ—¶è‡ªåŠ¨ä¿å­˜ç»Ÿè®¡æ•°æ®ï¼Œé›¶é¢å¤–è¯·æ±‚
"""

import argparse
import asyncio
import os
import re
import sqlite3
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List

# å¼ºåˆ¶ä½¿ç”¨è„šæœ¬æ‰€åœ¨ç›®å½•ä½œä¸ºå·¥ä½œç›®å½•
SKILL_DIR = Path(__file__).parent.parent.resolve()
os.chdir(SKILL_DIR)

DB_PATH = SKILL_DIR / "douyin_users.db"
F2_VIDEO_DB_PATH = SKILL_DIR / "douyin_videos.db"
CONFIG_PATH = SKILL_DIR / "config" / "config.yaml"


def create_metadata_table():
    """åˆ›å»ºè§†é¢‘å…ƒæ•°æ®è¡¨"""
    conn = sqlite3.connect(str(DB_PATH))
    cursor = conn.cursor()

    cursor.execute("""
        CREATE TABLE IF NOT EXISTS video_metadata (
            aweme_id TEXT PRIMARY KEY,
            uid TEXT NOT NULL,
            desc TEXT,
            create_time INTEGER,
            duration INTEGER,
            digg_count INTEGER DEFAULT 0,
            comment_count INTEGER DEFAULT 0,
            collect_count INTEGER DEFAULT 0,
            share_count INTEGER DEFAULT 0,
            play_count INTEGER DEFAULT 0,
            local_filename TEXT,
            file_size INTEGER,
            fetch_time INTEGER
        )
    """)

    cursor.execute("""
        CREATE INDEX IF NOT EXISTS idx_video_uid ON video_metadata(uid)
    """)

    cursor.execute("""
        CREATE INDEX IF NOT EXISTS idx_video_digg ON video_metadata(digg_count DESC)
    """)

    conn.commit()
    conn.close()
    print("âœ… å…ƒæ•°æ®è¡¨å·²åˆ›å»º/éªŒè¯")


def get_video_stats_from_f2_db() -> List[Dict]:
    """ä» F2 çš„è§†é¢‘æ•°æ®åº“è·å–ç»Ÿè®¡æ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰"""
    if not F2_VIDEO_DB_PATH.exists():
        return []

    conn = sqlite3.connect(str(F2_VIDEO_DB_PATH))
    cursor = conn.cursor()

    try:
        cursor.execute("""
            SELECT
                aweme_id, uid, desc, create_time, duration,
                digg_count, comment_count, collect_count, share_count
            FROM video_info
        """)
        rows = cursor.fetchall()
        conn.close()

        videos = []
        for row in rows:
            videos.append({
                "aweme_id": row[0],
                "uid": row[1] or "",
                "desc": row[2] or "",
                "create_time": int(row[3]) if row[3] else 0,
                "duration": int(row[4]) if row[4] else 0,
                "digg_count": int(row[5]) if row[5] else 0,
                "comment_count": int(row[6]) if row[6] else 0,
                "collect_count": int(row[7]) if row[7] else 0,
                "share_count": int(row[8]) if row[8] else 0,
            })
        return videos
    except sqlite3.OperationalError:
        conn.close()
        return []


def scan_local_videos() -> Dict[str, Dict]:
    """æ‰«ææœ¬åœ°è§†é¢‘æ–‡ä»¶ï¼Œè¿”å› aweme_id -> æ–‡ä»¶ä¿¡æ¯çš„æ˜ å°„"""
    downloads_path = SKILL_DIR / "downloads"
    if not downloads_path.exists():
        return {}

    videos = {}
    for video_file in downloads_path.rglob("*.mp4"):
        # æ–‡ä»¶åæ ¼å¼: {æ—¶é—´æˆ³}_{æè¿°}_{aweme_id}_video.mp4
        # æ³¨æ„ï¼šæè¿°ä¸­å¯èƒ½åŒ…å«ä¸‹åˆ’çº¿
        stem = video_file.stem

        # ç§»é™¤æœ«å°¾çš„ _video
        stem = re.sub(r'_video$', '', stem)

        # æ‰¾æ‰€æœ‰çº¯æ•°å­—æ®µ
        parts = stem.split("_")
        numeric_parts = []
        for part in parts:
            part = part.strip()
            if part.isdigit() and len(part) >= 15:
                numeric_parts.append(part)

        # å¦‚æœæ‰¾åˆ°çº¯æ•°å­—æ®µï¼Œè¿”å›æœ€é•¿çš„é‚£ä¸ª
        if numeric_parts:
            aweme_id = max(numeric_parts, key=len)
        else:
            # å›é€€æ–¹æ¡ˆï¼šä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ‰¾æœ€é•¿çš„æ•°å­—ä¸²
            matches = re.findall(r'\d{15,}', stem)
            if matches:
                aweme_id = max(matches, key=len)
            else:
                continue

        stat = video_file.stat()
        parent_dir = video_file.parent.name  # uid

        videos[aweme_id] = {
            "aweme_id": aweme_id,
            "uid": parent_dir,
            "local_filename": video_file.name,
            "file_size": stat.st_size,
        }

    return videos


def save_metadata(videos: List[Dict]):
    """ä¿å­˜å…ƒæ•°æ®åˆ°æ•°æ®åº“"""
    if not videos:
        print("âš ï¸ æ²¡æœ‰è§†é¢‘å…ƒæ•°æ®éœ€è¦ä¿å­˜")
        return

    conn = sqlite3.connect(str(DB_PATH))
    cursor = conn.cursor()

    fetch_time = int(datetime.now().timestamp())

    for video in videos:
        cursor.execute("""
            INSERT OR REPLACE INTO video_metadata
            (aweme_id, uid, desc, create_time, duration,
             digg_count, comment_count, collect_count, share_count, play_count,
             local_filename, file_size, fetch_time)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            video.get("aweme_id"),
            video.get("uid", ""),
            video.get("desc", ""),
            video.get("create_time", 0),
            video.get("duration", 0),
            video.get("digg_count", 0),
            video.get("comment_count", 0),
            video.get("collect_count", 0),
            video.get("share_count", 0),
            video.get("play_count", 0),
            video.get("local_filename", ""),
            video.get("file_size", 0),
            fetch_time
        ))

    conn.commit()
    conn.close()
    print(f"âœ… å·²ä¿å­˜ {len(videos)} æ¡è§†é¢‘å…ƒæ•°æ®")


def get_cookie_from_config() -> str:
    """ä»é…ç½®æ–‡ä»¶è¯»å– Cookie"""
    import yaml

    if CONFIG_PATH.exists():
        with open(CONFIG_PATH, "r", encoding="utf-8") as f:
            config = yaml.safe_load(f)
            return config.get("douyin", {}).get("cookie", "") or config.get("cookie", "")
    return ""


def get_sec_user_id_from_db(uid: str) -> str:
    """ä»æ•°æ®åº“è·å– sec_user_id"""
    try:
        conn = sqlite3.connect(str(DB_PATH))
        cursor = conn.cursor()
        cursor.execute("SELECT sec_user_id FROM user_info_web WHERE uid = ?", (uid,))
        result = cursor.fetchone()
        conn.close()
        return result[0] if result else ""
    except Exception:
        return ""


async def fetch_video_stats_from_user_page(sec_user_id: str, delay: float = 2.0) -> Dict[str, Dict]:
    """
    é€šè¿‡è®¿é—®ç”¨æˆ·ä¸»é¡µè·å–è§†é¢‘ç»Ÿè®¡æ•°æ®

    ä» API å“åº”ä¸­æ•è· aweme_list æ•°æ®

    Args:
        sec_user_id: ç”¨æˆ·çš„ sec_user_id
        delay: è¯·æ±‚å»¶è¿Ÿ

    Returns:
        aweme_id -> ç»Ÿè®¡æ•°æ®çš„å­—å…¸
    """
    try:
        from playwright.async_api import async_playwright
    except ImportError:
        print("âŒ éœ€è¦å®‰è£… playwright: pip install playwright && playwright install chromium")
        return {}

    stats_data = {}
    all_videos = []

    print("  å¯åŠ¨æµè§ˆå™¨...")

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
        )

        # åŠ è½½ Cookie
        cookie_str = get_cookie_from_config()
        if cookie_str:
            cookies = []
            for item in cookie_str.split(';'):
                item = item.strip()
                if '=' in item:
                    name, value = item.split('=', 1)
                    cookies.append({
                        "name": name.strip(),
                        "value": value.strip(),
                        "domain": ".douyin.com",
                        "path": "/"
                    })
            if cookies:
                await context.add_cookies(cookies)
                print(f"  å·²åŠ è½½ {len(cookies)} ä¸ª Cookie")

        # ç›‘å¬ API å“åº”
        async def handle_response(response):
            if 'aweme/post' in response.url:
                try:
                    data = await response.json()
                    aweme_list = data.get('aweme_list', [])
                    for v in aweme_list:
                        aweme_id = v.get('aweme_id', '')
                        stats = v.get('statistics', {})
                        if aweme_id:
                            all_videos.append({
                                "aweme_id": aweme_id,
                                "uid": v.get('author', {}).get('uid', ''),
                                "desc": v.get('desc', ''),
                                "create_time": v.get('create_time', 0),
                                "duration": v.get('duration', 0),
                                "digg_count": stats.get('digg_count', 0),
                                "comment_count": stats.get('comment_count', 0),
                                "collect_count": stats.get('collect_count', 0),
                                "share_count": stats.get('share_count', 0),
                                "play_count": stats.get('play_count', 0),
                            })
                except Exception:
                    pass

        page = await context.new_page()
        page.on('response', handle_response)

        url = f"https://www.douyin.com/user/{sec_user_id}"
        print(f"  è®¿é—®ç”¨æˆ·ä¸»é¡µ...")

        try:
            await page.goto(url, wait_until="domcontentloaded", timeout=60000)
            print("  é¡µé¢åŠ è½½å®Œæˆï¼Œç­‰å¾… API å“åº”...")

            # æ»šåŠ¨é¡µé¢ä»¥åŠ è½½æ›´å¤šè§†é¢‘
            for i in range(5):
                await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                await page.wait_for_timeout(2000)
                print(f"  æ»šåŠ¨åŠ è½½ä¸­... ({i+1}/5)")

            # å†ç­‰å¾…ä¸€ä¸‹ç¡®ä¿æ‰€æœ‰è¯·æ±‚å®Œæˆ
            await page.wait_for_timeout(3000)

        except Exception as e:
            print(f"  è®¿é—®é¡µé¢æ—¶å‡ºé”™: {e}")

        await browser.close()

    # æ•´ç†æ•°æ®
    for v in all_videos:
        stats_data[v['aweme_id']] = v

    print(f"  âœ… è·å–åˆ° {len(stats_data)} ä¸ªè§†é¢‘çš„ç»Ÿè®¡æ•°æ®")
    return stats_data


def get_stats_summary() -> Dict:
    """è·å–ç»Ÿè®¡æ‘˜è¦"""
    conn = sqlite3.connect(str(DB_PATH))
    cursor = conn.cursor()

    # è§†é¢‘æ€»æ•°
    cursor.execute("SELECT COUNT(*) FROM video_metadata")
    total_videos = cursor.fetchone()[0]

    # æœ‰ç»Ÿè®¡æ•°æ®çš„è§†é¢‘æ•°
    cursor.execute("SELECT COUNT(*) FROM video_metadata WHERE digg_count > 0")
    videos_with_stats = cursor.fetchone()[0]

    # æ€»ç‚¹èµæ•°
    cursor.execute("SELECT SUM(digg_count) FROM video_metadata")
    total_diggs = cursor.fetchone()[0] or 0

    # ç”¨æˆ·ç»Ÿè®¡
    cursor.execute("""
        SELECT uid, COUNT(*) as count, SUM(digg_count) as total_diggs
        FROM video_metadata
        GROUP BY uid
        ORDER BY total_diggs DESC
    """)
    user_stats = cursor.fetchall()

    conn.close()

    return {
        "total_videos": total_videos,
        "videos_with_stats": videos_with_stats,
        "total_diggs": total_diggs,
        "user_stats": user_stats
    }


def main():
    parser = argparse.ArgumentParser(description="è§†é¢‘å…ƒæ•°æ®æå–å·¥å…·")
    parser.add_argument("--fetch", action="store_true", help="ä»ç½‘é¡µè·å–ç»Ÿè®¡æ•°æ®ï¼ˆéœ€è¦ sec_user_idï¼‰")
    parser.add_argument("--stats", action="store_true", help="æ˜¾ç¤ºç»Ÿè®¡æ‘˜è¦")
    parser.add_argument("--uid", type=str, help="æŒ‡å®šç”¨æˆ· UID")
    args = parser.parse_args()

    print("=" * 50)
    print("è§†é¢‘å…ƒæ•°æ®æå–å·¥å…·")
    print("=" * 50)

    # åˆ›å»ºè¡¨
    create_metadata_table()

    if args.stats:
        # æ˜¾ç¤ºç»Ÿè®¡
        stats = get_stats_summary()
        print(f"\nğŸ“Š ç»Ÿè®¡æ‘˜è¦:")
        print(f"   æ€»è§†é¢‘æ•°: {stats['total_videos']}")
        print(f"   æœ‰ç»Ÿè®¡æ•°æ®çš„è§†é¢‘: {stats['videos_with_stats']}")
        print(f"   æ€»ç‚¹èµæ•°: {stats['total_diggs']:,}")
        print(f"\n   ç”¨æˆ·ç»Ÿè®¡:")
        for uid, count, diggs in stats['user_stats'][:10]:
            print(f"     {uid}: {count} è§†é¢‘, {diggs:,} ç‚¹èµ")
        return

    # æ‰«ææœ¬åœ°è§†é¢‘
    print("\nğŸ“ æ‰«ææœ¬åœ°è§†é¢‘æ–‡ä»¶...")
    local_videos = scan_local_videos()
    print(f"   æ‰¾åˆ° {len(local_videos)} ä¸ªæœ¬åœ°è§†é¢‘")

    if args.fetch:
        # å¼ƒç”¨è­¦å‘Š
        print("\n" + "=" * 60)
        print("âš ï¸  è­¦å‘Š: --fetch é€‰é¡¹å·²åºŸå¼ƒ")
        print("   æ¨èä½¿ç”¨ download-v2.py é‡æ–°ä¸‹è½½è§†é¢‘ï¼Œä¼šè‡ªåŠ¨ä¿å­˜ç»Ÿè®¡æ•°æ®")
        print("   ç”¨æ³•: python scripts/download-v2.py <ä¸»é¡µURL>")
        print("=" * 60)

        # ä½¿ç”¨ Playwright ä»ç”¨æˆ·ä¸»é¡µè·å–ç»Ÿè®¡æ•°æ®
        print("\nğŸŒ ä»ç”¨æˆ·ä¸»é¡µè·å–ç»Ÿè®¡æ•°æ®...")

        # è·å–ç”¨æˆ·çš„ sec_user_id
        if args.uid:
            uid = args.uid
        else:
            # ä»æœ¬åœ°è§†é¢‘æ¨æ–­ uid
            uids = set(v['uid'] for v in local_videos.values())
            if len(uids) == 1:
                uid = list(uids)[0]
            else:
                print(f"âŒ å‘ç°å¤šä¸ªç”¨æˆ·: {uids}ï¼Œè¯·ä½¿ç”¨ --uid æŒ‡å®š")
                return

        sec_user_id = get_sec_user_id_from_db(uid)
        if not sec_user_id:
            print(f"âŒ æœªæ‰¾åˆ°ç”¨æˆ· {uid} çš„ sec_user_id")
            return

        print(f"   ç”¨æˆ· UID: {uid}")
        print(f"   sec_user_id: {sec_user_id[:20]}...\n")

        stats_data = asyncio.run(fetch_video_stats_from_user_page(sec_user_id))

        if stats_data:
            # åˆå¹¶æœ¬åœ°ä¿¡æ¯å’Œ API æ•°æ®
            merged = []
            for aweme_id, local_info in local_videos.items():
                video = local_info.copy()
                if aweme_id in stats_data:
                    video.update(stats_data[aweme_id])
                merged.append(video)

            save_metadata(merged)
    else:
        # æ£€æŸ¥ F2 æ•°æ®åº“
        print("\nğŸ“Š æ£€æŸ¥ F2 è§†é¢‘æ•°æ®åº“...")
        f2_videos = get_video_stats_from_f2_db()
        print(f"   æ‰¾åˆ° {len(f2_videos)} æ¡ F2 æ•°æ®åº“è®°å½•")

        # åˆ›å»º F2 æ•°æ®çš„ç´¢å¼•
        f2_index = {v["aweme_id"]: v for v in f2_videos}

        # åˆå¹¶æ•°æ®
        merged = []
        for aweme_id, local_info in local_videos.items():
            video = local_info.copy()
            if aweme_id in f2_index:
                f2_data = f2_index[aweme_id]
                video.update({
                    "desc": f2_data.get("desc", ""),
                    "create_time": f2_data.get("create_time", 0),
                    "duration": f2_data.get("duration", 0),
                    "digg_count": f2_data.get("digg_count", 0),
                    "comment_count": f2_data.get("comment_count", 0),
                    "collect_count": f2_data.get("collect_count", 0),
                    "share_count": f2_data.get("share_count", 0),
                })
            merged.append(video)

        save_metadata(merged)

    # æ˜¾ç¤ºç»“æœ
    stats = get_stats_summary()
    print(f"\nğŸ“Š å½“å‰çŠ¶æ€:")
    print(f"   æ€»è§†é¢‘æ•°: {stats['total_videos']}")
    print(f"   æœ‰ç»Ÿè®¡æ•°æ®çš„è§†é¢‘: {stats['videos_with_stats']}")


if __name__ == "__main__":
    main()
